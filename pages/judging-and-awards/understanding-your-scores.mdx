import { Callout } from 'nextra/components'

# Understanding Your Scores

After the event ends, you can request your scores and feedback by sending an email to [wildhacks@northwestern.edu](mailto:wildhacks@northwestern.edu).

Each project is judged individually by 3-4 different judges. Judges are assigned so that they do not judge the same project more than once and to minimize the number of projects that the same group of 3 judges judge together.

## Score Breakdown

Here's a breakdown of what the scores in our response mean.

**Score**

This is your total score, scaled to be out of 100. This is the weighted average of the [project evaluation](/judging-and-awards/project-evaluation) categories below. This score is determined during round 1 of judging and is your primary score.

<Callout type="warning">
    To normalize for differences in standards across judges, while ensuring scores accurately reflect the quality of a project, [the formula described on this page](/judging-and-awards/scoring-formula) is applied to raw scores from Round 1 of judging.

    In short, some judges are more strict than others, while others are more lenient. However, simply adjusting each judge's score based on their average does not account for the possibility that the projects evaluated by one judge are overall stronger or weaker than those evaluated by another judge.
    
    For example, suppose one judge evaluates five very strong projects. If we only normalize for score differences between judges, these projects will appear weaker, as they are pulled down by the global average.
    
    This creates an issue of equity, as teams who perform very well and get unlucky, or very poorly and get lucky, will have scores that do not reflect their performance.

    The statistical model here is our best attempt to equalize the playing field and decrease the luck factor in your scores.
</Callout>

**Technical Complexity**

This is your average technical complexity score, scaled out of 100.

[Learn more about Technical Complexity →](/judging-and-awards/project-evaluation/#technical-complexity-30)

**Usefulness**

This is your average usefulness score, scaled out of 100.

[Learn more about Usefulness →](/judging-and-awards/project-evaluation/#usefulness-30)

**Originality**

This is your average originality score, scaled out of 100.

[Learn more about Originality →](/judging-and-awards/project-evaluation/#originality-20)

**Design**

This is your average design score, scaled out of 100.

[Learn more about Design →](/judging-and-awards/project-evaluation/#design-15)

**Presentation**

This is your average presentation score, scaled out of 100.

[Learn more about Presentation →](/judging-and-awards/project-evaluation/#presentation-5)

**Placement Score**

This is your placement score out of 100 and is based on the ranking of your project in round 2. This will be 0 if you were not in the top 10 or if you were not ranked in the top 3 by any judges. It will be greater than 0 otherwise.

**Comments**

These are comments, if any, left by judges on your project. They are only made during round 1.
